# -*- coding: utf-8 -*-
"""Copy of IS2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nc598fK3CfSSn_0OoSJxU7NaqtvdoRIK
"""

!pip install pyngrok
!ngrok authtoken YOUR_NGROK_TOKEN
!pip install -U bitsandbytes
!pip install transformers accelerate bitsandbytes flash-attn
import os
import threading
os.environ["FLASK_DEBUG"]="1"
from flask import Flask
from pyngrok import ngrok
from flask import request
app = Flask(__name__)
port = "5000"

public_url = ngrok.connect(port).public_url

app.config["BASE_URL"] = public_url

import time

model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
HF_TOKEN= "PUT_YOUR_HUGGING_FACE_TOKEN"
import os
os.environ["HF_TOKEN"]= HF_TOKEN

import transformers
import torch

class Utilities:
  def fileToString(self, inFilePath):
    with open(inFilePath, 'r') as file:
      data = file.read().replace('\n', '')
    return data

  def stringToFile(self, outFilePath, content):
    with open(outFilePath, "w") as text_file:
      text_file.write(content)

ut = Utilities()

class LlmManager:
  def __init__(self):
    self.bnb_config = transformers.BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    self.model = transformers.AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        quantization_config=self.bnb_config,
        token=HF_TOKEN,
    )

    self.tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_name,
        token=HF_TOKEN,
    )

    self.pipe = transformers.pipeline(
        "text-generation",
        model=self.model,
        tokenizer=self.tokenizer,
        pad_token_id=self.tokenizer.eos_token_id,
    )

    self.terminators = [
        self.pipe.tokenizer.eos_token_id,
        self.pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

  def executePrompt(self, prompt):
    messages = [
        {
            "role": "system",
            "content": prompt
        }
    ]
    outputs = self.pipe(
      messages,
      max_new_tokens=1024,
      eos_token_id=self.terminators,
      do_sample=True,
      temperature=0.6,
      top_p=0.9,
    )

    assistant_response = outputs[0]["generated_text"][-1]["content"]
    return assistant_response

llm_man=LlmManager()

@app.route("/promptLLM", methods=['POST', 'GET'])
def promptLLM():
  if request.method=='GET':
    return "Please use POST method. Arguments: prompt â€“ question to LLM"
  if request.method=='POST':
  	prompt = request.values.get('prompt')
  	answer = llm_man.executePrompt(prompt)
  return answer


print('Prompt:')
t = time.process_time()
print(llm_man.executePrompt("How are you?"))
elapsed_time = time.process_time() - t
print(elapsed_time)

print("You can access the service publicly at:")
print(public_url)

threading.Thread(target=app.run, kwargs={"use_reloader": False}).start()